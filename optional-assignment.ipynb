{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"nteract-front-end@1.0.0"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# REGRESSION\n\nConsider the following database from he UCI databases repository https://archive.ics.uci.edu/dataset/29/computer+hardware\n","metadata":{}},{"cell_type":"code","source":"# install the  b package\n\n!pip install ucimlrepo","metadata":{"execution":{"iopub.status.busy":"2024-05-09T12:29:21.409258Z","iopub.execute_input":"2024-05-09T12:29:21.409684Z","iopub.status.idle":"2024-05-09T12:29:37.851549Z","shell.execute_reply.started":"2024-05-09T12:29:21.409653Z","shell.execute_reply":"2024-05-09T12:29:37.850519Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting ucimlrepo\n  Downloading ucimlrepo-0.0.6-py3-none-any.whl.metadata (5.3 kB)\nDownloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"from ucimlrepo import fetch_ucirepo \nfrom sklearn.preprocessing import LabelEncoder\n\n# fetch dataset \ncomputer_hardware = fetch_ucirepo(id=29) \n  \n# data (as pandas dataframes) \ndf = computer_hardware.data.features\ndf.dropna(inplace=True)\n\nX = df.drop(['PRP', 'ERP'], axis=1)\ny = df[['PRP', 'ERP']]\nprint(df.shape)\nprint(df.dtypes)\n\nle = LabelEncoder()\nX['VendorName'] = le.fit_transform(X['VendorName'])\nX['ModelName'] = le.fit_transform(X['ModelName'])\n\n# metadata \n#print(computer_hardware.metadata) \n  \n# variable information \n#print(computer_hardware.variables) ","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:24:29.762917Z","iopub.execute_input":"2024-05-09T13:24:29.763327Z","iopub.status.idle":"2024-05-09T13:24:30.049927Z","shell.execute_reply.started":"2024-05-09T13:24:29.763295Z","shell.execute_reply":"2024-05-09T13:24:30.049115Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"(209, 10)\nVendorName    object\nModelName     object\nMYCT           int64\nMMIN           int64\nMMAX           int64\nCACH           int64\nCHMIN          int64\nCHMAX          int64\nPRP            int64\nERP            int64\ndtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The content of the Database are:\n\nFeatures\n1. vendor name: 30 \n      (adviser, amdahl,apollo, basf, bti, burroughs, c.r.d, cambex, cdc, dec, \n       dg, formation, four-phase, gould, honeywell, hp, ibm, ipl, magnuson, \n       microdata, nas, ncr, nixdorf, perkin-elmer, prime, siemens, sperry, \n       sratus, wang)\n2. Model Name: many unique symbols\n3. MYCT: machine cycle time in nanoseconds (integer)\n4. MMIN: minimum main memory in kilobytes (integer)\n5. MMAX: maximum main memory in kilobytes (integer)\n6. CACH: cache memory in kilobytes (integer)\n7. CHMIN: minimum channels in units (integer)\n8. CHMAX: maximum channels in units (integer)\n\nTarget \n\n9. PRP: published relative performance (integer)\n10. ERP: estimated relative performance from the original article (integer)\n\n\nPerform a regression using ANNs on the 1-8 features and compare your results with each of the target values. \n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# worse horrible when x is not scaled\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# works worse when y is scaled\n# scaler2 = StandardScaler()\n# y_train = scaler2.fit_transform(y_train)\n# y_test = scaler2.fit_transform(y_test)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train.values, dtype=torch.float32) \nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test.values, dtype=torch.float32) \nprint(X_train)\n\nclass RegressionModel(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(RegressionModel, self).__init__()\n        self.hidden1 = nn.Linear(input_size, 128) \n        self.hidden2 = nn.Linear(128, 64)\n        self.output_layer = nn.Linear(64, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.hidden1(x))\n        x = torch.relu(self.hidden2(x))\n        output = torch.relu(self.output_layer(x))\n        return output\n\ninput_size = X_train.shape[1]\noutput_size = y_train.shape[1]\nmodel = RegressionModel(input_size, output_size)\n\ncriterion = nn.L1Loss() # this is MAE\noptimizer = optim.Adam(model.parameters(), lr=0.01)  \n\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    output = model(X_train)\n    loss = criterion(output, y_train)\n    loss.backward()\n    optimizer.step()\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nwith torch.no_grad():\n    test_output = model(X_test)\n#     test_output = torch.tensor(scaler2.inverse_transform(test_output.numpy()))\n#     y_test_tensor = torch.tensor(scaler2.inverse_transform(y_test_tensor.numpy()))\n    print(test_output[0:5])\n    print(y_test[0:5])\n    test_loss = criterion(test_output, y_test)\n    print(f\"Test Loss: {test_loss.item():.4f}\")\n    \n# The model is overfitting. I tried solving this by adding L2 regularization (to keep the weight smaller) and \n# Dropout layers, but it didn't work. I believe the dataset is too small, which is probably true, since it contains only 209 samples. \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:25:39.100693Z","iopub.execute_input":"2024-05-09T13:25:39.101963Z","iopub.status.idle":"2024-05-09T13:25:40.738960Z","shell.execute_reply.started":"2024-05-09T13:25:39.101896Z","shell.execute_reply":"2024-05-09T13:25:40.738126Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([[-1.2403,  1.4201,  0.3916,  ..., -0.5159, -0.5472, -0.5159],\n        [-1.3723,  0.4312, -0.3738,  ...,  3.1823,  0.6599,  2.3147],\n        [ 0.4755,  1.2325, -0.5925,  ...,  0.0201,  0.3150, -0.0278],\n        ...,\n        [-0.0525,  0.8574, -0.2645,  ..., -0.4087, -0.5472,  0.1674],\n        [ 1.2674, -0.6259, -0.1916,  ..., -0.1943, -0.5472, -0.1254],\n        [ 0.2115, -1.2056,  3.2346,  ..., -0.6231, -0.5472, -0.7599]])\nEpoch [100/1000], Loss: 13.2488\nEpoch [200/1000], Loss: 9.7092\nEpoch [300/1000], Loss: 7.9636\nEpoch [400/1000], Loss: 6.3563\nEpoch [500/1000], Loss: 5.4501\nEpoch [600/1000], Loss: 4.5304\nEpoch [700/1000], Loss: 3.8267\nEpoch [800/1000], Loss: 3.6246\nEpoch [900/1000], Loss: 4.2676\nEpoch [1000/1000], Loss: 4.2726\ntensor([[362.7743, 102.1993],\n        [ 30.4442,  25.1430],\n        [ 16.0376,  24.8046],\n        [451.6995, 558.1439],\n        [ 16.4170,  34.4876]])\ntensor([[274., 102.],\n        [ 30.,  25.],\n        [ 22.,  25.],\n        [915., 919.],\n        [ 16.,  34.]])\nTest Loss: 32.8160\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}